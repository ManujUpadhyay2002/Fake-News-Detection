{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/train.csv')\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['title']+' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['author','title','text','id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "from spacy import displacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Drop rows with NaN values in 'content' column\n",
    "sample_df = sample_df.dropna(subset=['content'])\n",
    "\n",
    "# Tokenization and Lemmatization using spaCy\n",
    "sample_df['tokenized_content'] = sample_df['content'].apply(lambda x: ' '.join([token.lemma_ for token in nlp(x)]))\n",
    "\n",
    "# Split the DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_df['tokenized_content'], sample_df['label'], test_size=0.20, random_state=42)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# # Confusion Matrix Visualization\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.show()\n",
    "\n",
    "# # Word Cloud for Fake News\n",
    "# fake_news_words = \" \".join(sample_df[sample_df['label'] == 1]['tokenized_content'])\n",
    "# wordcloud = WordCloud(width=800, height=400, max_words=150, background_color='white').generate(fake_news_words)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.title('Word Cloud for Fake News')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "svc = SVC(kernel='sigmoid',gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear',penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50,random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50,random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50,random_state=2)\n",
    "gdbt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)\n",
    "\n",
    "clfs = {\n",
    "    'SVC' : svc,\n",
    "    'KN':knc,\n",
    "    'NB':mnb,\n",
    "    'DT':dtc,\n",
    "    'LR':lrc,\n",
    "    'RF':rfc,\n",
    "    'AdaBoost':abc,\n",
    "    'Bgc':bc,\n",
    "    'ETC':etc,\n",
    "    'GBDT':gdbt,\n",
    "    'xgb' : xgb\n",
    "}\n",
    "\n",
    "def train_classifier(clf,x_train,y_train,x_test,y_test):\n",
    "   clf.fit(x_train,y_train)\n",
    "   y_pred = clf.predict(x_test)\n",
    "   # Performance Metrics\n",
    "   accuracy = accuracy_score(y_test,y_pred)\n",
    "   precision = precision_score(y_test,y_pred)\n",
    "   recall = recall_score(y_test,y_pred)\n",
    "   f1 = f1_score(y_test,y_pred)\n",
    "   conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "\n",
    "   return accuracy,precision,recall,f1,conf_matrix\n",
    "\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrix_scores = []\n",
    "for name,clf in clfs.items():\n",
    "    current_accuracy,current_precision,current_recall,current_f1,current_conf_matrix = train_classifier(clf,X_train_tfidf,y_train,X_test_tfidf\n",
    ",y_test)\n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)\n",
    "    recall_scores.append(current_recall)\n",
    "    f1_scores.append(current_f1)\n",
    "    conf_matrix_scores.append(current_conf_matrix)\n",
    "\n",
    "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,\n",
    "'Precision':precision_scores,'Recall':recall_scores,'F1':f1_scores,'conf_matrix':conf_matrix_scores}).sort_values('Precision',ascending=False)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "    'Algorithm': ['NB', 'ETC', 'AdaBoost', 'GBDT', 'Bgc', 'xgb', 'DT', 'RF', 'SVC', 'LR', 'KN'],\n",
    "    'Accuracy': [0.692308, 0.861538, 0.943590, 0.943590, 0.923077, 0.933333, 0.887179, 0.841026, 0.851282, 0.830769, 0.646154],\n",
    "    'Precision': [1.000000, 0.946667, 0.936842, 0.936842, 0.934066, 0.926316, 0.860000, 0.846154, 0.835052, 0.790476, 0.588652],\n",
    "    'Recall': [0.361702, 0.755319, 0.946809, 0.946809, 0.904255, 0.936170, 0.914894, 0.819149, 0.861702, 0.882979, 0.882979],\n",
    "    'F1': [0.531250, 0.840237, 0.941799, 0.941799, 0.918919, 0.931217, 0.886598, 0.832432, 0.848168, 0.834171, 0.706383]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the Algorithm column as the index for better visualization\n",
    "df.set_index('Algorithm', inplace=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x=df['Accuracy'], y=df.index, palette='viridis')\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Precision\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x=df['Precision'], y=df.index, palette='viridis')\n",
    "plt.title('Precision')\n",
    "\n",
    "# Recall\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x=df['Recall'], y=df.index, palette='viridis')\n",
    "plt.title('Recall')\n",
    "\n",
    "# F1 Score\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.barplot(x=df['F1'], y=df.index, palette='viridis')\n",
    "plt.title('F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap for Confusion Matrices\n",
    "conf_matrices = [\n",
    "    [[101, 0], [60, 34]],\n",
    "    [[97, 4], [23, 71]],\n",
    "    [[95, 6], [5, 89]],\n",
    "    [[95, 6], [5, 89]],\n",
    "    [[95, 6], [9, 85]],\n",
    "    [[94, 7], [6, 88]],\n",
    "    [[87, 14], [8, 86]],\n",
    "    [[87, 14], [17, 77]],\n",
    "    [[85, 16], [13, 81]],\n",
    "    [[79, 22], [11, 83]],\n",
    "    [[43, 58], [11, 83]]\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i in range(len(conf_matrices)):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    sns.heatmap(conf_matrices[i], annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, linewidths=.5)\n",
    "    plt.title(f'Confusion Matrix - {df.index[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
